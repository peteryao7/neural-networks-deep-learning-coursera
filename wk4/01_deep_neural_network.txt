Deep L-Layer Neural Network

A deep NN is a NN that has more than one hidden layer.

A NN with a single hidden layer would be a 2-layer NN, only counting the hidden and output layers.
    - logistic regression is a 2-layer NN
    - these are also known as "shallow" NNs

-----

Notation
        O   O
x_1     O   O   O
x_2     O   O   O   O -> 킹
x_3     O   O   O
        O   O

        1   2   3   4

L = # layers = 4
n^[l] = # units in layer l
    n^[1] = 5, n^[2] = 5, n^[3] = 3, n^[4] = n^[L] = 1
    n^[0] = n_x = 3
a^[l] = activations in layer l
      = g^[l](z^[l])
W^[l] = weights for computing z^[l]
b^[l] also used to compute z^[l]
x = a^[0]
a[L] = 킹

==================================================

Forward Propagation in a Deep Network

For the hidden layers in a deep NN with 5 units and given a single training example x:
    Activations of layer 1:
        z^[1] = W^[1] * x + b^[1]
              = W^[1] * a^[0] + b^[1]
        a^[1] = g^[1](z^[1])
    Activations of layer 2:
        z^[2] = W^[2] * a^[1] + b^[2]
        a^[2] = g^[2](z^[2])
    ...
    Activations of the output layer:
        z^[4] = W^[4] * a^[3] + b^[4]
        a^[4] = g^[4](z^[4])
              = 킹

Note x = a^[0], so we can substitute that in the formula for x.

General formulas:
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

Vectorizing the formulas:
    Z^[1] = W^[1] * X + b^[1]
          = W^[1] * A^[0] + b^[1]
    A^[1] = g^[1](Z^[1])
    Z^[2] = W^[2] * A^[1] + b^[2]
    A^[2] = g^[2](Z^[2])

    킷 = g(Z^[4]) = A^[4]

Note X = A^[0], so we can substitute that in the formula for X.

Looking at the vectorized formulas, it looks like we need an explicit for loop to loop over the layers,
in this case for l = 1...4.
But there's no way to go around it, so looping around the layers is fine.

==================================================
6:35 - a^[l] = g^[l](z^[l])
Getting Your Matrix Dimensions Right