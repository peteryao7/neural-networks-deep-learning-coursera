Deep L-Layer Neural Network

A deep NN is a NN that has more than one hidden layer.

A NN with a single hidden layer would be a 2-layer NN, only counting the hidden and output layers.
    - logistic regression is a 2-layer NN
    - these are also known as "shallow" NNs

-----

Notation
        O   O
x_1     O   O   O
x_2     O   O   O   O -> ŷ
x_3     O   O   O
        O   O

        1   2   3   4

L = # layers = 4
n^[l] = # units in layer l
    n^[1] = 5, n^[2] = 5, n^[3] = 3, n^[4] = n^[L] = 1
    n^[0] = n_x = 3
a^[l] = activations in layer l
      = g^[l](z^[l])
W^[l] = weights for computing z^[l]
b^[l] also used to compute z^[l]
x = a^[0]
a[L] = ŷ

==================================================

Forward Propagation in a Deep Network

For the hidden layers in a deep NN with 5 units and given a single training example x:
    Activations of layer 1:
        z^[1] = W^[1] * x + b^[1]
              = W^[1] * a^[0] + b^[1]
        a^[1] = g^[1](z^[1])
    Activations of layer 2:
        z^[2] = W^[2] * a^[1] + b^[2]
        a^[2] = g^[2](z^[2])
    ...
    Activations of the output layer:
        z^[4] = W^[4] * a^[3] + b^[4]
        a^[4] = g^[4](z^[4])
              = ŷ

Note x = a^[0], so we can substitute that in the formula for x.

General formulas:
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

Vectorizing the formulas:
    Z^[1] = W^[1] * X + b^[1]
          = W^[1] * A^[0] + b^[1]
    A^[1] = g^[1](Z^[1])
    Z^[2] = W^[2] * A^[1] + b^[2]
    A^[2] = g^[2](Z^[2])

    Ŷ = g(Z^[4]) = A^[4]

Note X = A^[0], so we can substitute that in the formula for X.

Looking at the vectorized formulas, it looks like we need an explicit for loop to loop over the layers,
in this case for l = 1...4.
But there's no way to go around it, so looping around the layers is fine.

==================================================
6:35 - a^[l] = g^[l](z^[l])
a and z have dimensions (n^[l],1)
Getting Your Matrix Dimensions Right

Working through the dimensions of your variables is a good way to debug your code

-----

Ex. NN with n^[0] = n_x = 2, n^[1] = 3, n^[2] = 5, n^[3] = 4, n^[4] = 2, n^[5] = 1

z^[1] = W^[1] * x + b^[1]
    z^[1] ∈ R^(3 x 1), or R^(n^[1] x 1)
    x ∈ R^(2 x 1), or R^(n^[0] x 1)
    So we need W^[1] ∈ R^(3 x 2)
    W^[1] ∈ R^(n^[1] x n^[0])

Since z^[1] ∈ R^(3 x 1), b^[1] also needs to be a 3 x 1 vector

For z^[2] = W^[2] * a^[1] + b^[2]
    z^[2] ∈ R^(5 x 1)
    W^[2] ∈ R^(5 x 3)
    a^[1] ∈ R^(3 x 1)

Since z^[2] is 5 x 1, b^[2] needs to be a 5 x 1 vector
    
W^[3] ∈ R^(4 x 5)
W^[4] ∈ R^(2 x 4)
W^[5] ∈ R^(1 x 2)

Generally: 
W^[l] ∈ R^(n^[l], n^[l-1])
b^[l] ∈ R^(n^[l], 1)
dW^[l] ∈ R^(n^[l], n^[l-1])
db^[l] ∈ R^(n^[l], 1)

For z^[l] = g^[l](a^[l]), a^[l] should have the same dimensions as g^[l]

-----

Vectorized implmementation

Z^[1] = W^[1] * X + b^[1]
where Z^[1] is now a horizontal stack of z^[1](1...m) column vectors
and X is all the training examples stack horizontally
b^[1] is still (n^[1], 1), but that's ok since Python will broadcast it and make dupe columns to make it 
(n^[1], m), then added element-wise

Z^[l], A^[l] ∈ R^(n^[l], m)
When l = 0, A^[0] = X = (n^[0], m)

dZ^[l], dA^[l] ∈ R^(n^[l], m)
