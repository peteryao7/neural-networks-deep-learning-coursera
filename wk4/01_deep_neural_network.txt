Deep L-Layer Neural Network

A deep NN is a NN that has more than one hidden layer.

A NN with a single hidden layer would be a 2-layer NN, only counting the hidden and output layers.
    - logistic regression is a 2-layer NN
    - these are also known as "shallow" NNs

-----

Notation
        O   O
x_1     O   O   O
x_2     O   O   O   O -> ŷ
x_3     O   O   O
        O   O

        1   2   3   4

L = # layers = 4
n^[l] = # units in layer l
    n^[1] = 5, n^[2] = 5, n^[3] = 3, n^[4] = n^[L] = 1
    n^[0] = n_x = 3
a^[l] = activations in layer l
      = g^[l](z^[l])
W^[l] = weights for computing z^[l]
b^[l] also used to compute z^[l]
x = a^[0]
a[L] = ŷ

==================================================

Forward Propagation in a Deep Network

For the hidden layers in a deep NN with 5 units and given a single training example x:
    Activations of layer 1:
        z^[1] = W^[1] * x + b^[1]
              = W^[1] * a^[0] + b^[1]
        a^[1] = g^[1](z^[1])
    Activations of layer 2:
        z^[2] = W^[2] * a^[1] + b^[2]
        a^[2] = g^[2](z^[2])
    ...
    Activations of the output layer:
        z^[4] = W^[4] * a^[3] + b^[4]
        a^[4] = g^[4](z^[4])
              = ŷ

Note x = a^[0], so we can substitute that in the formula for x.

General formulas:
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

Vectorizing the formulas:
    Z^[1] = W^[1] * X + b^[1]
          = W^[1] * A^[0] + b^[1]
    A^[1] = g^[1](Z^[1])
    Z^[2] = W^[2] * A^[1] + b^[2]
    A^[2] = g^[2](Z^[2])

    Ŷ = g(Z^[4]) = A^[4]

Note X = A^[0], so we can substitute that in the formula for X.

Looking at the vectorized formulas, it looks like we need an explicit for loop to loop over the layers,
in this case for l = 1...4.
But there's no way to go around it, so looping around the layers is fine.

==================================================

Getting Your Matrix Dimensions Right

Working through the dimensions of your variables is a good way to debug your code

-----

Ex. NN with n^[0] = n_x = 2, n^[1] = 3, n^[2] = 5, n^[3] = 4, n^[4] = 2, n^[5] = 1

z^[1] = W^[1] * x + b^[1]
    z^[1] ∈ R^(3 x 1), or R^(n^[1] x 1)
    x ∈ R^(2 x 1), or R^(n^[0] x 1)
    So we need W^[1] ∈ R^(3 x 2)
    W^[1] ∈ R^(n^[1] x n^[0])

Since z^[1] ∈ R^(3 x 1), b^[1] also needs to be a 3 x 1 vector

For z^[2] = W^[2] * a^[1] + b^[2]
    z^[2] ∈ R^(5 x 1)
    W^[2] ∈ R^(5 x 3)
    a^[1] ∈ R^(3 x 1)

Since z^[2] is 5 x 1, b^[2] needs to be a 5 x 1 vector
    
W^[3] ∈ R^(4 x 5)
W^[4] ∈ R^(2 x 4)
W^[5] ∈ R^(1 x 2)

Generally: 
W^[l] ∈ R^(n^[l], n^[l-1])
b^[l] ∈ R^(n^[l], 1)
dW^[l] ∈ R^(n^[l], n^[l-1])
db^[l] ∈ R^(n^[l], 1)

For a^[l] = g^[l](z^[l]), a^[l] should have the same dimensions as g^[l]

-----

Vectorized implmementation

Z^[1] = W^[1] * X + b^[1]
where Z^[1] is now a horizontal stack of z^[1](1...m) column vectors
and X is all the training examples stack horizontally
b^[1] is still (n^[1], 1), but that's ok since Python will broadcast it and make dupe columns to make it 
(n^[1], m), then added element-wise

Z^[l], A^[l] ∈ R^(n^[l], m)
When l = 0, A^[0] = X = (n^[0], m)

dZ^[l], dA^[l] ∈ R^(n^[l], m)

==================================================

Why Deep Representations?

Why do NNs need deep networks, or lots of hidden layers?

-----

Say your input is a picture of a face.

The first layer of the NN is like a feature/edge detector.

In this example, there are 20 hidden units visualized by little gray squares.
One hidden unit is figuring out where the edge is in the image.
One hidden unit is trying to find the horizontal edges in the image.
etc.

Next, once it figures out where the edges are, then it can group edges together to form parts of faces.
A neuron might try to find an eye, or a part of a nose, or the mouth etc.

Finally, once that hidden layer puts together different parts of faces, now it can
start to detect faces in the next hidden layer.

First, the network tries to find simple things, then using that to figure out more complex things.

-----

Ex. speech recognition system

Audio -> low level audio wave form features (is the tone going up/down? is there white noise? silence?)
      -> basic units of sounds (phonemes) 
      -> words
      -> phrases and sentences


Deep NNs are compared to the human brain, where the brain starts off detecting simple things like
edges in what your eyes see, then builds up to detect more complex things like faces. 

-----

Circuit theory and deep learning

Informally: There are functions you can compute with a "small" L-layer deep NN that shallower networks
require exponentially more hidden units to compute.

Say you're trying to compute x_1 XOR x_2 XOR x_3 ... XOR x_n

x_1 ->
        XOR ->
x_2 ->          
                XOR -> y
x_3 ->
        XOR ->
x_4 ->

The depth of the network is O(log(n)) and the number of nodes/circuit components/# gates in the network
isn't that big.

If you're not allowed to use a NN with log(n) hidden layers,

x_1 -> O ->
x_2 -> O ->
x_3 -> O -> O -> y
...
x_n -> O ->

Now you need a hidden layer with 2^n nodes to compute XOR, since you need to enumerate all 2^n possible
configurations of the input bits that result in the XOR being 0 or 1.

To find the right depth of your NN, try using 1 or 2 hidden layers at first, then expanding if you
need more.

==================================================

Building Blocks of Deep Neural Networks

Given a network with a few layers, select a layer.

For layer l, we have params W^[l] and b^[l]
For FP: input a^[l-1], output a^[l]
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

    Cache z^[l] for later.

For BP: input da^[l], in cache z^[l], output da^[l-1], dw^[l], and db^[l]

Once you finish implementing FP and BP, then the basic computation of the NN is:
a^[0] -> W^[1], b^[1] -> a^[1] -> W^[2], b^[2] -> a^[2] -> ... -> a^[L] = ŷ
              v                        v                           v
         cache z^[1]              cache z^[2]                  cache z^[L]
              v                        v                           v
         W^[1], b^[1] <- da^[1]<- W^[2], b^[2] <- da^[2]<- ... <- W^[L], b^[L] <- da^[L]
           dz^[1]                   dz^[2]                                                   
              v                                                     v
            dW^[1]                                                dw^[L]
            db^[1]                                                db^[L]

W^[l] = W^[l] - α * dw^[l]
b^[l] = b^[l] - α * db^[l]
