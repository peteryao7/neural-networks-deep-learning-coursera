Deep L-Layer Neural Network

A deep NN is a NN that has more than one hidden layer.

A NN with a single hidden layer would be a 2-layer NN, only counting the hidden and output layers.
    - logistic regression is a 2-layer NN
    - these are also known as "shallow" NNs

-----

Notation
        O   O
x_1     O   O   O
x_2     O   O   O   O -> ŷ
x_3     O   O   O
        O   O

        1   2   3   4

L = # layers = 4
n^[l] = # units in layer l
    n^[1] = 5, n^[2] = 5, n^[3] = 3, n^[4] = n^[L] = 1
    n^[0] = n_x = 3
a^[l] = activations in layer l
      = g^[l](z^[l])
W^[l] = weights for computing z^[l]
b^[l] also used to compute z^[l]
x = a^[0]
a[L] = ŷ

==================================================

Forward Propagation in a Deep Network

For the hidden layers in a deep NN with 5 units and given a single training example x:
    Activations of layer 1:
        z^[1] = W^[1] * x + b^[1]
              = W^[1] * a^[0] + b^[1]
        a^[1] = g^[1](z^[1])
    Activations of layer 2:
        z^[2] = W^[2] * a^[1] + b^[2]
        a^[2] = g^[2](z^[2])
    ...
    Activations of the output layer:
        z^[4] = W^[4] * a^[3] + b^[4]
        a^[4] = g^[4](z^[4])
              = ŷ

Note x = a^[0], so we can substitute that in the formula for x.

General formulas:
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

Vectorizing the formulas:
    Z^[1] = W^[1] * X + b^[1]
          = W^[1] * A^[0] + b^[1]
    A^[1] = g^[1](Z^[1])
    Z^[2] = W^[2] * A^[1] + b^[2]
    A^[2] = g^[2](Z^[2])

    Ŷ = g(Z^[4]) = A^[4]

Note X = A^[0], so we can substitute that in the formula for X.

Looking at the vectorized formulas, it looks like we need an explicit for loop to loop over the layers,
in this case for l = 1...4.
But there's no way to go around it, so looping around the layers is fine.

==================================================

Getting Your Matrix Dimensions Right

Working through the dimensions of your variables is a good way to debug your code

-----

Ex. NN with n^[0] = n_x = 2, n^[1] = 3, n^[2] = 5, n^[3] = 4, n^[4] = 2, n^[5] = 1

z^[1] = W^[1] * x + b^[1]
    z^[1] ∈ R^(3 x 1), or R^(n^[1] x 1)
    x ∈ R^(2 x 1), or R^(n^[0] x 1)
    So we need W^[1] ∈ R^(3 x 2)
    W^[1] ∈ R^(n^[1] x n^[0])

Since z^[1] ∈ R^(3 x 1), b^[1] also needs to be a 3 x 1 vector

For z^[2] = W^[2] * a^[1] + b^[2]
    z^[2] ∈ R^(5 x 1)
    W^[2] ∈ R^(5 x 3)
    a^[1] ∈ R^(3 x 1)

Since z^[2] is 5 x 1, b^[2] needs to be a 5 x 1 vector
    
W^[3] ∈ R^(4 x 5)
W^[4] ∈ R^(2 x 4)
W^[5] ∈ R^(1 x 2)

Generally: 
W^[l] ∈ R^(n^[l], n^[l-1])
b^[l] ∈ R^(n^[l], 1)
dW^[l] ∈ R^(n^[l], n^[l-1])
db^[l] ∈ R^(n^[l], 1)

For a^[l] = g^[l](z^[l]), a^[l] should have the same dimensions as g^[l]

-----

Vectorized implmementation

Z^[1] = W^[1] * X + b^[1]
where Z^[1] is now a horizontal stack of z^[1](1...m) column vectors
and X is all the training examples stack horizontally
b^[1] is still (n^[1], 1), but that's ok since Python will broadcast it and make dupe columns to make it 
(n^[1], m), then added element-wise

Z^[l], A^[l] ∈ R^(n^[l], m)
When l = 0, A^[0] = X = (n^[0], m)

dZ^[l], dA^[l] ∈ R^(n^[l], m)

==================================================

Why Deep Representations?

Why do NNs need deep networks, or lots of hidden layers?

-----

Say your input is a picture of a face.

The first layer of the NN is like a feature/edge detector.

In this example, there are 20 hidden units visualized by little gray squares.
One hidden unit is figuring out where the edge is in the image.
One hidden unit is trying to find the horizontal edges in the image.
etc.

Next, once it figures out where the edges are, then it can group edges together to form parts of faces.
A neuron might try to find an eye, or a part of a nose, or the mouth etc.

Finally, once that hidden layer puts together different parts of faces, now it can
start to detect faces in the next hidden layer.

First, the network tries to find simple things, then using that to figure out more complex things.

-----

Ex. speech recognition system

Audio -> low level audio wave form features (is the tone going up/down? is there white noise? silence?)
      -> basic units of sounds (phonemes) 
      -> words
      -> phrases and sentences


Deep NNs are compared to the human brain, where the brain starts off detecting simple things like
edges in what your eyes see, then builds up to detect more complex things like faces. 

-----

Circuit theory and deep learning

Informally: There are functions you can compute with a "small" L-layer deep NN that shallower networks
require exponentially more hidden units to compute.

Say you're trying to compute x_1 XOR x_2 XOR x_3 ... XOR x_n

x_1 ->
        XOR ->
x_2 ->          
                XOR -> y
x_3 ->
        XOR ->
x_4 ->

The depth of the network is O(log(n)) and the number of nodes/circuit components/# gates in the network
isn't that big.

If you're not allowed to use a NN with log(n) hidden layers,

x_1 -> O ->
x_2 -> O ->
x_3 -> O -> O -> y
...
x_n -> O ->

Now you need a hidden layer with 2^n nodes to compute XOR, since you need to enumerate all 2^n possible
configurations of the input bits that result in the XOR being 0 or 1.

To find the right depth of your NN, try using 1 or 2 hidden layers at first, then expanding if you
need more.

==================================================

Building Blocks of Deep Neural Networks

Given a network with a few layers, select a layer.

For layer l, we have params W^[l] and b^[l]
For FP: input a^[l-1], output a^[l]
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

    Cache z^[l] for later.

For BP: input da^[l], in cache z^[l], output da^[l-1], dw^[l], and db^[l]

Once you finish implementing FP and BP, then the basic computation of the NN is:
a^[0] -> W^[1], b^[1] -> a^[1] -> W^[2], b^[2] -> a^[2] -> ... -> a^[L] = ŷ
              v                        v                           v
         cache z^[1]              cache z^[2]                  cache z^[L]
              v                        v                           v
         W^[1], b^[1] <- da^[1]<- W^[2], b^[2] <- da^[2]<- ... <- W^[L], b^[L] <- da^[L]
           dz^[1]                   dz^[2]                                                   
              v                                                     v
            dW^[1]                                                dw^[L]
            db^[1]                                                db^[L]

W^[l] = W^[l] - α * dw^[l]
b^[l] = b^[l] - α * db^[l]

==================================================

Forward and Backward Propagation

FP for layer l, input a^[l-1], output a^[l], cache z^[l]
    z^[l] = W^[l] * a^[l-1] + b^[l]
    a^[l] = g^[l](z^[l])

FP vectorized:
    Z^[l] = W^[l] * A^[l-1] + b^[l] // b is broadcasted
    A^[l] = g^[l](z^[l])

BP for layer l, input da^[l]. output da^[l-1], dW^[l], and db^[l]
    dz^[l] = da^[l] * g^[l]'(z^[l])
    dw^[l] = dz^[l] * a^[l-1]^T
    db^[l] = dz^[l]
    da^[l-1] = W^[l]^T * dz^[l]

If you take the definition of da^[l] and put it in the equation for dz^[l], then you get:
    dz^[l] = W^[l+1]^Tdz^[l+1] * g^[l]'(z^[l])

BP vectorized:
    dZ^[l] = dA^[l] * g^[l]'(Z^[l])
    dW^[l] = 1/m * dZ^[l] * A^[l-1]^T
    db^[l] = 1/m * np.sum(dZ^[l], axis=1, keepdims=True)
    dA^[l-1] = W^[L]^T * dZ^[l]

-----

Summary

x ->  ReLU  ->  ReLU  -> sigmoid -> ŷ -> ℒ(ŷ,y)
        v        v          v
      z^[1]    z^[2]      z^[3]
        v        v          v
  <-        <-        <-         <- da^[1]
da^[0]    da^[1]    da^[2]
        v        v          v
     dW^[1]    dW^[2]     dW^[3]
     db^[1]    db^[2]     db^[3]

da^[1] = -y/a + (1-y)/(1-a)
dA^[l] = (-y^(1) / a^(1) + (1-y^(1)) / (1 - a^(1)) ... -y^(m) / a^(m) + (1-y^(m)) / (1 - a^(m)))

Sometimes, when you implement a ML algorithm, there's a lot of magic because a lot of the information
comes from the data instead of the code you wrote. It's not a lot of code, but you can feed it so
much data that it's still surprising that the algorithm works. Much of the complexity comes from
the data instead of thousands of lines of code.

==================================================

Parameters vs Hyperparameters

Parameters for our model: W^[1], b^[1], W^[2], b^[2], ...

Hyperparameters for your learning algorithm: 
    - learning rate α
        - determines how your parameters evolve
        - how many iterations of GD you carry out
    - # hidden layers L
    - # hidden units n^[1], n^[2], ...
    - choice of activation function (ReLU, sigmoid, tanh)

These hyperparameters control the parameters and determine the final values of W and b.

Later, more hyperparameters: momentum term, mini-batch size, regularization terms, ...

-----

Applied deep learning is a very empirical process (try out a lot of things and see what works)

Idea -> Code -> Experiment -> Idea -> Code -> Experiment -> etc.

Idea: the best value for α is 0.01
Code: implement it and try it out
Experiment: plot # iterations vs. cost J
Idea: α is too low, maybe try 0.05
etc.

Then you find out in your experiment that J decreases at a decent rate compared to the # iterations,
so that's the α you want.

There are a lot of different hybrid parameters, and it's hard to know in advance the best values of
the hyperparameters, so you just have to guess at first and go through that cycle, then iterate.

Deep learning nowadays is used in a lot of ML fields, like vision, speech, NLP, online advertising,
web search, product recommendations etc.

It's also possible that the best values for the hyperparameters today will change later on; maybe
the technologies you're using, like the GPU and CPU, are different and α needs to be tuned again.

==================================================

What Does This Have to Do With the Human Brain?

Not a whole lot.

FP and BP equations:
    dZ^[L] = A^[L] - Y
    dW^[L] = 1/m * dZ^[L] * A^[L-1]^T
    db^[L] = 1/m * np.sum(dZ^[L], axis=1, keepdims=True)
    dZ^[L-1] = W^[L]^TdZ^[L] * g'^[L-1](Z^[L-1])

    Where * denotes element-wise multiplication

    ...

    dZ^[1] = W^[2]dZ^[2] * g'^[1](Z^[1])
    dW^[1] = 1/m * dZ^[1]A^[0]^T

    Note that A^[0]^T is another way to denote X^T

    db^[1] = 1/m * np.sum(dZ^[1], axis=1, keepdims=True)

Say there's a logistic regression with a sigmoid activation function. It looks quite similar
to a neuron in your nervous system, where there's an input, then sends a pulse of electricity
down to the axon, then a long wire down to other neurons. 

But today, even neuroscientists believe a neuron is a lot more complex than that and still
aren't fully sure what they're doing.

We also still don't know whether humans are using an algorithm to learn and whether we use
FP and BP in our learning process.