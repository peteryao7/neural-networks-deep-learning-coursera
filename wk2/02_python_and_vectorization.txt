Vectorization

The art of getting rid of explicit folders in your code.
It's important that your code runs quickly on a big data set, else it'll take a long time
to get any kind of result.

-----

What is it?

In logistic regression, you need to calculate z = w^T*x+b, and w and x can be long feature vectors,
where w ∈ R^(n_x) and x ∈ R^(n_x)

In a non-vectorized method:
z = 0
for i in range(n_x):
    z += w[i] * x[i]
z += b

In a vectorized implementation:
z = np.dot(w,x) + b // np.dot computes w^T*x

-----

Jupyter demo

import numpy as np

a = np.array([1,2,3,4])
print(a)

(shift+Enter executes code)

---

import time

a = np.random.rand(1000000)
b = rp.random.rand(1000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()

print("Vectorized version:" + str(1000 * (toc - tic)) + "ms") // multiply by 1000 since time.time is in seconds
// prints ~1.478 ms

c = 0
tic = time.time()
for i in range(1000000):
    c += a[i] * b[i]
toc = time.time()

print("For loop:" + str(1000 * (toc - tic)) + "ms")
// prints ~474.295 ms for the same calculation, about 300x longer

-----

A lot of scaleable deep learning implementations are done on a GPU, but the demos done on Jupyter use the CPU.
But it turns out both the GPU and CPU have parallelization instructions, or SIMD instructions.
If you built-in functions like the np. functions, it enables numpy to take much better advantage of
parallelism. GPUs are amazing at SIMD calculations, but the CPU isn't bad at it either.

==================================================

More Vectorization Examples

NN programming guideline

Whenever possible, avoid explicit for loops
Say you want to compute u=A*v where A is a matrix and u a vector
By the definition of matrix multiplication:
    u = np.zeros((n,1))
    for i...
        for j...
            u[i] += A[i][j] * v[j]
We have 2 for loops.

With a vectorized version:
u = np.dot(A,v)
Way faster.

-----

Say you need to apply the exponential operation on every element of a matrix/vector:
v = [v_1; v_2; ...; v_n] and you want u = [e^v_1; ...; e^v_n]

Non-vectorized:
u = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])

Vectorized with a built-in numpy function:
import numpy at np
u = np.exp(v)
// np has other useful methods for math, like np.log(v), np.abs(v), np.maximum(v,0), 
// v ** 2 (element-wise square of each elt in v), 1/v etc.

-----

Logistic regression derivatives

In our previous code, we had a for loop and 2 features inside. If we had a lot more features, we'd
need an inner for loop to go over all the features.

Instead of the second for loop, we can vectorize the calculations instead of calculating dw_1 and dw_2
that way.

dw = np.zeros((n_x,1))
dw += x^(i) * dz^(i)

dw /= m to take the average.

==================================================

Vectorizing Logistic Regression

Implementing GD without a for loop

Want to compute z^(i) = w^T*x^(i) + b, a^(i) = σ(z^(i)) for each training example

X = [x^(1); x^(2); ...; x^(m)] with a column for each x, X ∈ R^(n_x x m)
[z^(1) z^(2) ... z^(m)] = w^T * X + [b b ... b], where the b matrix is a 1 x m row vector
w^T is also a row vector of size 1 x n_x, and w^T * X results in a 1 x m row vector

Once you calculate everything, you get all the z values for z^(1), z^(2), ..., z^(m) 
in a single row matrix, stacked horizontally.

Z = np.dot(w.T, x) + b // w.T is the transpose and b is a real number, not a row vector
Python will convert b to a row vector automatically, called broadcasting.

Now to compute a^(1), a^(2), ..., a^(m)
We will define a row vector A = [a^(1) a^(2) ... a^(m)] = σ(Z), and calculate all a^(i)'s at the same time.

==================================================

Vectorizing Logistic Regression's Gradient Computation

We computed GD by:
dz^(i) = a^(i) - y^(i) for i = 1,...,m

Again, put it all in a matrix:
dZ = [dz^(1) dz^(2) ... dz^(m)]

We know how to calculate A = [a^(1) ... a^(m)], and we also defined Y = [y^(1) ... y^(m)]

Therefore, we can just calculate all of dZ as so:
dZ = A - Y = [a^(1)-y^(1) ... a^(m)-y^(m)]
with no loops.

Previously, we had to loop through all our training examples in GD. We removed the inner for loop over our
features by using an accumulator, but we can vectorize the rest of the process in a similar fashion.
    db = 1/m * sum(i=1, m) dz^(i)
       = 1/m * np.sum(dZ) // in Python
    dw = 1/m * X * dZ.T

No more for loops!

-----

Implementing vectorized logistic regression

Z = np.dot(w.T, X) + b
A = sigmoid(Z)
dZ = A - Y
dw = 1/m * X * dZ.T
db = 1/m * np.sum(dZ)

This implements FP and BP on all m training examples without a for loop.

GD update (still requires a loop):
for i in range(m):
    w := w - α*dw
    b := b - α*db
