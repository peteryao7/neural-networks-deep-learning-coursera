Vectorization

The art of getting rid of explicit folders in your code.
It's important that your code runs quickly on a big data set, else it'll take a long time
to get any kind of result.

-----

What is it?

In logistic regression, you need to calculate z = w^T*x+b, and w and x can be long feature vectors,
where w ∈ R^(n_x) and x ∈ R^(n_x)

In a non-vectorized method:
z = 0
for i in range(n_x):
    z += w[i] * x[i]
z += b

In a vectorized implementation:
z = np.dot(w,x) + b // np.dot computes w^T*x

-----

Jupyter demo

import numpy as np

a = np.array([1,2,3,4])
print(a)

(shift+Enter executes code)

---

import time

a = np.random.rand(1000000)
b = rp.random.rand(1000000)

tic = time.time()
c = np.dot(a,b)
toc = time.time()

print("Vectorized version:" + str(1000 * (toc - tic)) + "ms") // multiply by 1000 since time.time is in seconds
// prints ~1.478 ms

c = 0
tic = time.time()
for i in range(1000000):
    c += a[i] * b[i]
toc = time.time()

print("For loop:" + str(1000 * (toc - tic)) + "ms")
// prints ~474.295 ms for the same calculation, about 300x longer

-----

A lot of scaleable deep learning implementations are done on a GPU, but the demos done on Jupyter use the CPU.
But it turns out both the GPU and CPU have parallelization instructions, or SIMD instructions.
If you built-in functions like the np. functions, it enables numpy to take much better advantage of
parallelism. GPUs are amazing at SIMD calculations, but the CPU isn't bad at it either.

==================================================

More Vectorization Examples

NN programming guideline

Whenever possible, avoid explicit for loops
Say you want to compute u=A*v where A is a matrix and u a vector
By the definition of matrix multiplication:
    u = np.zeros((n,1))
    for i...
        for j...
            u[i] += A[i][j] * v[j]
We have 2 for loops.

With a vectorized version:
u = np.dot(A,v)
Way faster.

-----

Say you need to apply the exponential operation on every element of a matrix/vector:
v = [v_1; v_2; ...; v_n] and you want u = [e^v_1; ...; e^v_n]

Non-vectorized:
u = np.zeros((n,1))
for i in range(n):
    u[i] = math.exp(v[i])

Vectorized with a built-in numpy function:
import numpy at np
u = np.exp(v)
// np has other useful methods for math, like np.log(v), np.abs(v), np.maximum(v,0), 
// v ** 2 (element-wise square of each elt in v), 1/v etc.

-----

Logistic regression derivatives

In our previous code, we had a for loop and 2 features inside. If we had a lot more features, we'd
need an inner for loop to go over all the features.

Instead of the second for loop, we can vectorize the calculations instead of calculating dw_1 and dw_2
that way.

dw = np.zeros((n_x,1))
dw += x^(i) * dz^(i)

dw /= m to take the average.

==================================================

Vectorizing Logistic Regression

Implementing GD without a for loop

Want to compute z^(i) = w^T*x^(i) + b, a^(i) = σ(z^(i)) for each training example

X = [x^(1); x^(2); ...; x^(m)] with a column for each x, X ∈ R^(n_x x m)
[z^(1) z^(2) ... z^(m)] = w^T * X + [b b ... b], where the b matrix is a 1 x m row vector
w^T is also a row vector of size 1 x n_x, and w^T * X results in a 1 x m row vector

Once you calculate everything, you get all the z values for z^(1), z^(2), ..., z^(m) 
in a single row matrix, stacked horizontally.

Z = np.dot(w.T, x) + b // w.T is the transpose and b is a real number, not a row vector
Python will convert b to a row vector automatically, called broadcasting.

Now to compute a^(1), a^(2), ..., a^(m)
We will define a row vector A = [a^(1) a^(2) ... a^(m)] = σ(Z), and calculate all a^(i)'s at the same time.

==================================================

Vectorizing Logistic Regression's Gradient Computation

We computed GD by:
dz^(i) = a^(i) - y^(i) for i = 1,...,m

Again, put it all in a matrix:
dZ = [dz^(1) dz^(2) ... dz^(m)]

We know how to calculate A = [a^(1) ... a^(m)], and we also defined Y = [y^(1) ... y^(m)]

Therefore, we can just calculate all of dZ as so:
dZ = A - Y = [a^(1)-y^(1) ... a^(m)-y^(m)]
with no loops.

Previously, we had to loop through all our training examples in GD. We removed the inner for loop over our
features by using an accumulator, but we can vectorize the rest of the process in a similar fashion.
    db = 1/m * sum(i=1, m) dz^(i)
       = 1/m * np.sum(dZ) // in Python
    dw = 1/m * X * dZ.T

No more for loops!

-----

Implementing vectorized logistic regression

Z = np.dot(w.T, X) + b
A = sigmoid(Z)
dZ = A - Y
dw = 1/m * X * dZ.T
db = 1/m * np.sum(dZ)

This implements FP and BP on all m training examples without a for loop.

GD update (still requires a loop):
for i in range(m):
    w := w - α*dw
    b := b - α*db

==================================================

Broadcasting in Python

Example: calories from carbs, proteins, fats in 100g of different foods

       apples    beef    eggs    potatoes
carb [  56.0      0.0     4.4      68.0   ]
prot [  1.2      104.0   52.0       8.0   ]     = A ∈ R^(3x4)
fat  [  1.8      135.0   99.0       0.9   ]
Sum:    59.0     139.0   155.4     76.9

We want to calculate % calories from carbs, proteins, and fats
For example, 56/59 ≈ 94.9%, so about 95% of an apple's calories come from carbs
Can you do this without an explicit for loop in Python?

-----

import numpy as np
A = np.array([56.0, 0.0, 4.4, 68.0], 
             [1.2,104.0,52.0,8.0],
             [1.8,135.0,99.0,0.9])

cal = A.sum(axis=0) // calculates the column sums
    // cal = [59. 239. 155.4 76.9]
percentage = 100 * A / cal.reshape(1,4)
    // cal is technically already a 1x4 matrix, so reshape isn't necessary here
    // but reshape will modify the dimensions of the matrix so you can perform the necessary operations on it
    // we get our result matrix

axis = 0: summing columns
axis = 1: summing rows

-----

More broadcasting examples

[1;2;3;4] + 100 = [1;2;3;4] + [100;100;100;100] = [101;102;103;104]

[1 2 3; 4 5 6] + [100 200 300]
    (m x n)         (1 x n)
Python will copy the second matrix m times to turn it from 1xn to mxn. 
It copies the row and appends new rows, so now the addition is:
[1 2 3; 4 5 6] + [100 200 300; 100 200 300] = [101 202 303; 104 205 306]

[1 2 3; 4 5 6] + [100; 200]
    (m x n)        (m x 1)
Python will copy the second matrix n times to turn it from mx1 to mxn.
It copies the column and appends new columns, so now the addition is:
[1 2 3; 4 5 6] + [100 100 100; 200 200 200] = [101 102 103; 204 205 206]

-----

General Principle

For an (m,n) matrix, if you want to perform the operations +-*/ on vectors (1,n) or (m,1), 
then the vectors will be broadcasted to an (m,n) matrix.

For a (m,1) vector, for the same operations +-*/ on a real number, then
the number will be turned into a (m,1) vector.

Matlab/Octave: bsxfun is a similar function to broadcasting in Python

==================================================

A Note on Python/numpy Vectors

Broadcasting can introducing strange bugs if you aren't aware of how it really works.
You might expect a calculation to not work without broadcasting and return an error, but
sometimes it can return valid output because of broadcasting, but is actually wrong.

-----

import numpy as np

a = np.random.randn(5) // 5 random Gaussian variables, NOT a vector [# # # # #]
a.shape // gives the length of a rank 1 array, not a vector
print(a.T) // ends up looking the same as a [# # # # #]
np.dot(a,a.T) // returns a number, not a matrix

Instead set a = np.random.randn(5,1) to get a column vector instead.
Now a.T returns a 1x5 row vector [[# # # # #]] (note the double brackets)
np.dot(a,a.T) // 5x5 matrix

-----

A rank 1 array is not a row or column vector. Avoid using them.
Instead, create vectors by specifying 2 parameters as your vector dimensions.

a = np.random.randn(5,1) returns a 5x1 column vector, where a.shape = (5,1)
a = np.random.randn(1,5) returns a 1x5 row vector, where a.shape = (1,5)
assert(a.shape == (m,n)) to check vector dimensions. Can also serve as documentation. 

If you do end up with a rank 1 array, then you can call reshape on it so it can behave correctly.

==================================================

Quick Tour of Jupyter/iPython Notebooks

Run your imports by running the cell
Restart the kernel if it's slow
