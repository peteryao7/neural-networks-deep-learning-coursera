Neural Networks Overview

Think of the computation graphs from last week. Neural networks are similar, but we
are stacking some more sigmoid units.

x_1     O

x_2     O   ->   O  ->  ŷ

x_3     O

The first hidden layer
x

W^[1] -> z^[1] = W^[1]*x + b^[1]  -> a^[1] = σ(z^[1]) -> z^[2] = W^[2]*a^[1] + b^[2] -> a^[2] = σ(z^[2])  ->  ℒ(a^[2],y)
                                                          ^ with W^[2], b^[2]
b^[1]

Whereas for logistic regression, we had a z and a calculation, we have multiple z and a calculations
to calculate the loss function. 

Same thing for using BP to calculate da and dz backwards.

==================================================

NN Representation

NN with a single hidden layer:
        O
x_1
        O
x_2          O -> ŷ
        O
x_3
        O

Input layer: the first layer (the x's)
Hidden layer: the layers you don't see in the training set (the one and only one is the 4 O's)
Output layer: the last layer (the lone O)

Values of input features denatoed as a^[0] = x
a also stands for activations, refers to the values that the different layers are passing on to subsequent layers
a^[1] is comprised of the values a_1^[1], a_2^[1], a_3^[1], a_4^[1] from the hidden layer in a column vector
a^[2] is just a real number, so ŷ = a^[2]

The above example is also known as a 2 layer NN. The layers counted are the hidden and output layers, not the
inner layer.

The hidden and output layers have parameters associated with them.
    - hidden layers have w^[i], b^[i] params, in this example w^[1] is a 4x3 matrix and b^[1] a 4x1 matrix
    - the output layer has params w^[2] of dimension 1x4 and b^[2] of dimension 1x1

==================================================

Computing a NN's Output

In logistic regression, a "node" represents 2 calculations for z and a.
A NN just does that many more times.

-----

Going back to our 2 layer NN example:

The first node in our 4-node hidden layer will calculate z_1^[1] = w^[1]^T*x+b^[1] and a_1^[1] = σ(z_1^[1])
The subscript number is the node in the layer and the bracket superscript is the layer

The second node in the 4-node hidden layer calculates z_2^[1] = w_2^[1]^T*x + b_2^[1] and a_2^[1] = σ(z_2^[1])

For the last 2 nodes:
    z_3^[1] = w_3^[1]^T*x + b_3^[1] and a_3^[1] = σ(z_3^[1])
    z_4^[1] = w_4^[1]^T*x + b_4^[1] and a_4^[1] = σ(z_4^[1])

Now to vectorize the computations for z and a:
    z^[1] = W^[1] * x + b^[1]
    where W^[1] ∈ R^(4x3), x ∈ R(3x1), b^[1] ∈ R^(4x1)

    a^[1] = σ(z^[1])

    z^[2] = W^[2] * a^[1] + b^[2]
    where W^[1] ∈ R^(1x4), b^[2] ∈ R^(1x1)
    a^[2] = σ(z^[2])

Note that x = a^[0], so you could substitute the x in z^[1] with a^[0].
