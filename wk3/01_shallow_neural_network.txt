Neural Networks Overview

Think of the computation graphs from last week. Neural networks are similar, but we
are stacking some more sigmoid units.

x_1     O

x_2     O   ->   O  ->  ŷ

x_3     O

The first hidden layer
x

W^[1] -> z^[1] = W^[1]*x + b^[1]  -> a^[1] = σ(z^[1]) -> z^[2] = W^[2]*a^[1] + b^[2] -> a^[2] = σ(z^[2])  ->  ℒ(a^[2],y)
                                                          ^ with W^[2], b^[2]
b^[1]

Whereas for logistic regression, we had a z and a calculation, we have multiple z and a calculations
to calculate the loss function. 

Same thing for using BP to calculate da and dz backwards.

==================================================

NN Representation

NN with a single hidden layer:
        O
x_1
        O
x_2          O -> ŷ
        O
x_3
        O

Input layer: the first layer (the x's)
Hidden layer: the layers you don't see in the training set (the one and only one is the 4 O's)
Output layer: the last layer (the lone O)

Values of input features denatoed as a^[0] = x
a also stands for activations, refers to the values that the different layers are passing on to subsequent layers
a^[1] is comprised of the values a_1^[1], a_2^[1], a_3^[1], a_4^[1] from the hidden layer in a column vector
a^[2] is just a real number, so ŷ = a^[2]

The above example is also known as a 2 layer NN. The layers counted are the hidden and output layers, not the
inner layer.

The hidden and output layers have parameters associated with them.
    - hidden layers have w^[i], b^[i] params, in this example w^[1] is a 4x3 matrix and b^[1] a 4x1 matrix
    - the output layer has params w^[2] of dimension 1x4 and b^[2] of dimension 1x1

==================================================

Computing a NN's Output

In logistic regression, a "node" represents 2 calculations for z and a.
A NN just does that many more times.

-----

Going back to our 2 layer NN example:

The first node in our 4-node hidden layer will calculate z_1^[1] = w^[1]^T*x+b^[1] and a_1^[1] = σ(z_1^[1])
The subscript number is the node in the layer and the bracket superscript is the layer

The second node in the 4-node hidden layer calculates z_2^[1] = w_2^[1]^T*x + b_2^[1] and a_2^[1] = σ(z_2^[1])

For the last 2 nodes:
    z_3^[1] = w_3^[1]^T*x + b_3^[1] and a_3^[1] = σ(z_3^[1])
    z_4^[1] = w_4^[1]^T*x + b_4^[1] and a_4^[1] = σ(z_4^[1])

Now to vectorize the computations for z and a:
    z^[1] = W^[1] * x + b^[1]
    where W^[1] ∈ R^(4x3), x ∈ R(3x1), b^[1] ∈ R^(4x1)

    a^[1] = σ(z^[1])

    z^[2] = W^[2] * a^[1] + b^[2]
    where W^[1] ∈ R^(1x4), b^[2] ∈ R^(1x1)
    a^[2] = σ(z^[2])

Note that x = a^[0], so you could substitute the x in z^[1] with a^[0].

==================================================

Vectorizing Across Multiple Examples

With little modification, the equations we used last time can be used to compute outputs on
every example at the same time.

x     ---> a^[2] = ŷ
x^(1) ---> a^[2]^(1) = ŷ^(1)
x^(2) ---> a^[2]^(2) = ŷ^(2)
...
x^(n) ---> a^[2]^(m) = ŷ^(m)

Note: [i] is the layer number, and (i) is the example

for i = 1 to m:
    z^[1](i) = w^[1] * x^(1) + b^[1]
    a^[1](i) = σ(z^[1](i))
    z^[2](i) = w^[2] * a^[1](i) + b^[2]
    a^[2](i) = σ(z^[2](i))
end

Now to vectorize the loop for propagation:
    X = [x^(1) x^(2) ... x^(m)]
    Z^[1] = W^[1] * X + b^[1]
    A^[1] = σ(z^[1])
    Z^[2] = W^[2] * A^(1) + b^[2]
    A^[2] = σ(Z^[2])

We stacked up our examples into their own columns to make matrix X.
Then we store Z^[1], Z^[2] and A^[1], A^[2] in the same way, by stacking each individual vector horizontally.

For matrix A, horizontally it goes through different training examples, and vertically goes through 
different hidden units.

==================================================

Explanation for Vectorized Implementation

Say for the first training example, we calculate z^[1](1) = W^[1] * x^(1) + b^[1],
for the second training example, z^[1](2) = W^[1] * x^(2) + b^[1],
and for the third, z^[1](3) = W^[1] * x^(3) + b^[1]

Assume b = 0. Then W^[1] is a matrix and x is a vector, so W^[1]*x^(1) will give you a column vector,
then W^[1]*x^(2) gives you another column vector, and W^[1]*x^(3) gives a third column vector.

We're essentially stacking those x vectors into one giant X matrix. Then if we multiply W^[1] by X,
then the product vectors calculated individually will also stack horizontally because of how matrix
multiplication works, and we'll get a giant matrix Z^[1] holding z^[1](1), z^[1](2), ..., z^[1](m).

Note that x = A^[0], so x^(i) = a^[0](i), so there's a certain symmetry to the equations where
Z^[1] = W^[1] * A^[0] + b^[1].

==================================================

Activation Functions

In FP, we use the sigmoid function on z^[1] to calculate a^[1] and a^[2], where the function is
a = 1 / (1 + e^-z)

The sigmoid function goes between 0 and 1, but we can also use the hyperbolic tangent function,
a = tanh(z) = (e^z - e^-z) / (e^z + e^-z) which goes between -1 and +1.
It's essentially the sigmoid function shifted down to cross the origin.

If you set the sigmoid function σ(z^[1]) = tanh(z^[1]), it almost always works better than the sigmoid
function because the values between +1 and -1, the mean of the activations from the hidden layer
will typically have a mean closer to 0. It centers your data closer to 0 than around 0.5, making
learning for the next layer a bit easier.

In practice, the sigmoid function is strictly superior to the tanh function, except for the output layer,
since you want an output between 0 and 1 rather than between -1 and 1. 

For that reason, you may want to stick to the sigmoid function for binary classification as well.

One downside for both functions: if z is either very large or very small, then the gradient/derivative
of the function is very small, being close to 0, and slow down GD.
    - we can use the rectify linear unit, ReLU, a = max(0,z)
    - so the derivative is 1 when z > 0 and 0 when z < 0 (not differentiable at z = 0)
    - if you don't know what activation function to use, use ReLU
    - in practice, most of your hidden units will be z > 0, so learning can still be fast for 
      most training examples

Leaky ReLU
    - instead of setting the derivative of z < 0 to 0, it has a very slight slope
    - usually works better than ReLU, but not used as much in practice

-----

Pros and cons of activation functions

Sigmoid
    - a = 1 / (1 + e^-z)
    - never use this except for the output layer or binary classification
tanh
    - a = (e^z - e^-z) / (e^z + e^-z)
    - strictly superior to sigmoid in all other cases
ReLU
    - a = max(0,z)
    - if you're not sure what to use, use this one
Leaky ReLU
    - a = max(0.01*z,z)

-----

There are a lot of choices of how to build your NN, depending on how you choose # hidden layers, 
activation function, initialization etc.

If you're not sure what works best, try them all out and evaluate on a development set, and see
which one works better, then go with that.

==================================================

Why Do You Need Non-linear Activation Functions?

For a NN to compute interesting functions, an activation function is necessary.

Say when calculating a^[1] in our first hidden layer, set a^[1] = z^[1], where g(z) = z.
If you do this, then the model is just computing ŷ as a linear function of your input
features, x. 

If a^[1] = z^[1] = w^[1]*x + b^[1]
a^[2] = z^[2] = w^[2]*a^[1] + b^[2]
              = w^[2]*(w^[1]*x + b^[1]) + b^[2]
              = (w^[2]*w^[1])*x + (w^[2]*b^[1] + b^[2])
              = W'*x + b'

If you were to use linear/identity activation functions, then the NN is just outputting a linear function
of the input. No matter how many layers your NN has, you're just computing a linear function, so you
may as well have zero hidden layers and use logistic regression instead.

One place you may want to use a linear activation function g(z) = z is if you're doing ML on the 
regression problem. 
If y ∈ R, like the price of a house, then it may be ok to have a linear activation function so you
can ensure ŷ is also a real number going from -inf to inf. This is in the output layer only.

That doesn't mean the hidden units shouldn't be using a linear activation function. Use ReLU or tanh.

==================================================

Derivatives of Activation Functions

When you implement BP for your NN, you need to compute the derivation of the activation functions.

-----

For the sigmoid function, if you take any value of z, there will be some slope corresponding to its value.
d/dz g(z) = slope of g(x) at z
          = (1/(1+e^-z)) * (1-1/(1+e^-z))
          = g(z) * (1-g(z))
          = g'(z)

If z = 10, g(z) ≈ 1, then d/dz g(z) ≈ 1 * (1-1) ≈ 0
If z = -10, g(z) ≈ 0, then d/dz g(z) ≈ 0 * (1-0) ≈ 0
If z = 0, g(z) = 1/2, then d/dz g(z) = 1/2 * (1-1/2) = 1/4

Also, if a = g(z), then we can rewrite g'(z) as a * (1-a)

-----

For the tanh activation function: g(z) = tanh(z) = (e^z - e^-z) / (e^z + e^-z)

g'(z) = 1 - (tanh(z))^2

If z = 10, g(z) ≈ 1, then g'(z) ≈ 0
If z = -10, g(z) ≈ -1, then g'(z) ≈ 1 - (-1)^2 ≈ 0
If z = 0, g(z) = 0, then g'(z) = 1 - 0^2 = 1

-----

For ReLU and leaky ReLU

ReLU:
g(z) = max(0,z)
g'(z) = 0 if z < 0
        1 if z > 0
        not differentiable if z = 0, but you can set it to be 0 or 1 either way

Leaky ReLU:
g(z) = max(0.01*z,z)
g'(z) = 0.01 if z < 0
        1 if z > 0
        not differentiable if z = 0, but you can set it to be 0.01 or 1 either way

==================================================

GD for NN

The equations needed to compute BP here, intuition in the next section.

-----

Parameters: w^[1], b^[1], w^[2], b^[2]
For n_x = n^[0] input features, n^[1] hidden units, n^[2] = output units (1 in our case)
Then w^[1] ∈ R^(n^[1] x n^[0]), b^[1] ∈ R^(n^[1] x 1), w^[2] ∈ R^(n^[2] x n^[1]), b^[2] ∈ R^(n^[2] x 1)

Cost function: J(w^[1], b^[1], w^[2], b^[2]) = 1/m * sum(i=1, n) ℒ(ŷ,y), where ŷ = a^[2]

Gradient descent:
Initialize parameters randomly
repeat {
    compute predictions (ŷ^(i), i=1,...,m)
    dw^[1] = ∂J/∂w^[1], db^[1] = ∂J/∂b^[1], dw^[2] = ∂J/∂w^[2], db^[2] = ∂J/∂b^[2]
    w^[1] := w^[1] - α*dw^[1]
    b^[1] := b^[1] - α*db^[1]
    w^[2] := w^[2] - α*dw^[2]
    b^[2] := b^[2] - α*db^[2]
}
repeat until your params look like they're converging

But how do we compute the dw and db terms?

-----

Formulas for computing derivatives

For FP:
    Z^[1] = W^[1] * X + b^[1]
    A^[1] = g^[1] * (Z^[1])
    Z^[2] = W^[2] * A^[1] + b^[2]
    A^[2] = g^[2] * (Z^[2])

For BP (derivatives):
    // Let Y = [y^(1) y^(2) ... y^(m)], a 1 x m matrix listing your m examples horizontally
    dZ^[2] = A^[2] - Y
    dW^[2] = 1/m * dZ^[2] * A^[1]^T
    db^[2] = 1/m * np.sum(dZ^[2], axis = 1, keepdims = True) 
    // axis = 1 so we're summing horizontally 
    // keepdims prevents Python from outputing the funny rank 1 arrays where the dimension was (n,)
    dZ^[1] = (W^[2]^T * dZ^[2]) * (g^[1]' * Z^[1]) // the middle * is an element-wise product, or dot product
    dW^[1] = 1/m * dZ^[1] * X^T
    db^[1] = 1/m * np.sum(dZ^[1], axis = 1, keepdims = True)

Instead of using keepdims, we could call reshape on np.sum to form the sum into the correct dimension.

==================================================
8:20 - dw[1] instead of dw[2]
Backpropagation Intuition

x   ->
w   ->  z = w^T*x + b  ->  a = σ(z)  ->  ℒ(a,y)
b   ->

In logistic regression, we derived:
    da = -y/a + (1-y) / (1-a)
    dz = a - y
       = da * g'(z)
    dℒ/dz = dℒ/da * da/dz = g'(z)

The derivation for NN is very similar, but we do it twice since we have x going to a hidden layer,
then going to an output layer. 

-----

Neural network gradients

x
W^[1] -> z^[1] = W^[1]*x+b^[1] -> a^[1] = σ(z^[1]) -> z^[2] = W^[2]*a^[1]+b^[2] -> a^[2] = σ(z^[2]) -> ℒ(a^[2],y)
b^[1]                                                   ^
                                                   W^[2], b^[2]
BP goes backwards to compute:
    da^[2] collapsed with dz^[2], not needed to explicitly calculate
    dz^[2] = a^[2] - y
    dW^[2] = dz^[2]*a^[1]^T
        dw = dz*x, where w^[1] is a row vector, so a^[1] needs to be turned into a column vector
    db^[2] = dz^[2]
    da^[1] collapsed with dz^[1]
    dz^[1] = (W^[2]^T*dz^[2]) * (g^[1]'*z^[1]) // where middle * is element-wise multiplication
        W^[2] ∈ R^(n^[2] x n^[2]), z^[2], dz^[2] ∈ R^(n^[2] x 1), or 1x1 in binary classification
        z^[1], dz^[1] ∈ R^(n^[1] x 1)
        So dz^[1] ∈ R(n^[1] x 1)
    dW^[1] = dz^[1] * x^T
        x^T = a^[0]^T, so it's identical to dW^[2]
    db^[1] = dz^[1]

-----

Vectorized implementation of GD:
    z^[1] = W^[1]*x + b^[1]
    a^[1] = g^[1](z^[1])
    Z^[1] = [z^[1](1) z^[1](2) ... z^[1](m)] // stacking columns horizontally
    Z^[1] = W^[1]*X + b^[1]
    A^[1] = g^[1](Z^[1])

The same trick also works for BP:
    dZ^[2] = A^[2] - Y
    dW^[2] = 1/m * dZ^[2] * A^[1]^T // 1/m is from the cost function J
    db^[2] = 1/m * np.sum(dZ^[2], axis=1, keepdims=True)
    dZ^[1] = W^[2]^T(dZ^[2]) * g^[1]'(Z^[1])
    dW^[1] = 1/m * dZ^[1] * X^T
    db^[1] = 1/m * np.sum(dZ^[1], axis=1, keepdims=True)

==================================================

Random Initialization (for NN weights)

What if you initialize the weights in your NN to 0?
n^[0] = 2, n^[1] = 2
Set W^[1] = [0 0; 0 0], b^[1] = [0; 0]

Setting the bias terms b = 0 are ok, but for w it's a problem.
    - for any example you give it, you have a_1^[1] = a_2^[1]
    - when you compute BP, dz_1^[1] = dz_2^[2]
    - both hidden units are now completely identical/symmetric since they're computing the same function
    - by proof of induction, after every single iteration of training, your 2 hidden units are still
      computing the same function
    - dW = [u v; u v], so when W^[1] = W^[1] - α*dW, the first row is equal to the second row
    - therefore, there's no point in having more than one hidden unit

Instead, you want to initialize W randomly.
    - W^[1] = np.random.randn((2,2)) * 0.01
    - b^[1] = np.zero((2,1))
    - W^[2] = np.random.randn((1,2)) * 0.01
    - b^[2] = 0

Why multiply by 0.01 instead of 100?
    - we want the weights to be very small random values
    - if you're using a tanh or sigmoid activation function, if the weights are too large, then z will 
      become very large, and you're more likely to end up at the ends of the function, where the
      slope is close to 0
    - your activation function becomes saturated

There can be better constants than 0.01.
    - if your NN is shallow, without many hidden layers, then 0.01 is ok
    - but for a deep NN, you may want to pick a different, but still small, constant