Neural Networks Overview

Think of the computation graphs from last week. Neural networks are similar, but we
are stacking some more sigmoid units.

x_1     O

x_2     O   ->   O  ->  ŷ

x_3     O

The first hidden layer
x

W^[1] -> z^[1] = W^[1]*x + b^[1]  -> a^[1] = σ(z^[1]) -> z^[2] = W^[2]*a^[1] + b^[2] -> a^[2] = σ(z^[2])  ->  ℒ(a^[2],y)
                                                          ^ with W^[2], b^[2]
b^[1]

Whereas for logistic regression, we had a z and a calculation, we have multiple z and a calculations
to calculate the loss function. 

Same thing for using BP to calculate da and dz backwards.

==================================================

NN Representation

NN with a single hidden layer:
        O
x_1
        O
x_2          O -> ŷ
        O
x_3
        O

Input layer: the first layer (the x's)
Hidden layer: the layers you don't see in the training set (the one and only one is the 4 O's)
Output layer: the last layer (the lone O)

Values of input features denatoed as a^[0] = x
a also stands for activations, refers to the values that the different layers are passing on to subsequent layers
a^[1] is comprised of the values a_1^[1], a_2^[1], a_3^[1], a_4^[1] from the hidden layer in a column vector
a^[2] is just a real number, so ŷ = a^[2]

The above example is also known as a 2 layer NN. The layers counted are the hidden and output layers, not the
inner layer.

The hidden and output layers have parameters associated with them.
    - hidden layers have w^[i], b^[i] params, in this example w^[1] is a 4x3 matrix and b^[1] a 4x1 matrix
    - the output layer has params w^[2] of dimension 1x4 and b^[2] of dimension 1x1

==================================================

Computing a NN's Output

In logistic regression, a "node" represents 2 calculations for z and a.
A NN just does that many more times.

-----

Going back to our 2 layer NN example:

The first node in our 4-node hidden layer will calculate z_1^[1] = w^[1]^T*x+b^[1] and a_1^[1] = σ(z_1^[1])
The subscript number is the node in the layer and the bracket superscript is the layer

The second node in the 4-node hidden layer calculates z_2^[1] = w_2^[1]^T*x + b_2^[1] and a_2^[1] = σ(z_2^[1])

For the last 2 nodes:
    z_3^[1] = w_3^[1]^T*x + b_3^[1] and a_3^[1] = σ(z_3^[1])
    z_4^[1] = w_4^[1]^T*x + b_4^[1] and a_4^[1] = σ(z_4^[1])

Now to vectorize the computations for z and a:
    z^[1] = W^[1] * x + b^[1]
    where W^[1] ∈ R^(4x3), x ∈ R(3x1), b^[1] ∈ R^(4x1)

    a^[1] = σ(z^[1])

    z^[2] = W^[2] * a^[1] + b^[2]
    where W^[1] ∈ R^(1x4), b^[2] ∈ R^(1x1)
    a^[2] = σ(z^[2])

Note that x = a^[0], so you could substitute the x in z^[1] with a^[0].

==================================================

Vectorizing Across Multiple Examples

With little modification, the equations we used last time can be used to compute outputs on
every example at the same time.

x     ---> a^[2] = ŷ
x^(1) ---> a^[2]^(1) = ŷ^(1)
x^(2) ---> a^[2]^(2) = ŷ^(2)
...
x^(n) ---> a^[2]^(m) = ŷ^(m)

Note: [i] is the layer number, and (i) is the example

for i = 1 to m:
    z^[1](i) = w^[1] * x^(1) + b^[1]
    a^[1](i) = σ(z^[1](i))
    z^[2](i) = w^[2] * a^[1](i) + b^[2]
    a^[2](i) = σ(z^[2](i))
end

Now to vectorize the loop for propagation:
    X = [x^(1) x^(2) ... x^(m)]
    Z^[1] = W^[1] * X + b^[1]
    A^[1] = σ(z^[1])
    Z^[2] = W^[2] * A^(1) + b^[2]
    A^[2] = σ(Z^[2])

We stacked up our examples into their own columns to make matrix X.
Then we store Z^[1], Z^[2] and A^[1], A^[2] in the same way, by stacking each individual vector horizontally.

For matrix A, horizontally it goes through different training examples, and vertically goes through 
different hidden units.

==================================================

Explanation for Vectorized Implementation

Say for the first training example, we calculate z^[1](1) = W^[1] * x^(1) + b^[1],
for the second training example, z^[1](2) = W^[1] * x^(2) + b^[1],
and for the third, z^[1](3) = W^[1] * x^(3) + b^[1]

Assume b = 0. Then W^[1] is a matrix and x is a vector, so W^[1]*x^(1) will give you a column vector,
then W^[1]*x^(2) gives you another column vector, and W^[1]*x^(3) gives a third column vector.

We're essentially stacking those x vectors into one giant X matrix. Then if we multiply W^[1] by X,
then the product vectors calculated individually will also stack horizontally because of how matrix
multiplication works, and we'll get a giant matrix Z^[1] holding z^[1](1), z^[1](2), ..., z^[1](m).

Note that x = A^[0], so x^(i) = a^[0](i), so there's a certain symmetry to the equations where
Z^[1] = W^[1] * A^[0] + b^[1].
